{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829f93c9-48d0-455e-9dc5-6b40ec03345f",
   "metadata": {},
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db96390-bfc7-4068-973c-0a8624d4553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def templates_from_csv(filename):\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = ','.join(lines[i].split(',')[1:])\n",
    "        lines[i] = [\n",
    "            _.rstrip().lstrip()\n",
    "            for _ in lines[i].replace('\"', \"<*>\").split('<*>')\n",
    "            if any(letter in _ for letter in letters)\n",
    "        ]\n",
    "    return lines[1:]\n",
    "\n",
    "\n",
    "def get_cluster_id(line, templates_uniques):\n",
    "    uniques = []\n",
    "    for cluster_id, unique in enumerate(templates_uniques):\n",
    "        if all(u in line for u in unique):\n",
    "            uniques.append(cluster_id)\n",
    "    if len(uniques) == 1:\n",
    "        return uniques[0]\n",
    "    elif len(uniques) > 1:\n",
    "        for unique in uniques:\n",
    "            if all(\n",
    "                    all(\n",
    "                        _ in templates_uniques[unique]\n",
    "                        for _ in templates_uniques[other_unique]\n",
    "                    ) for other_unique in uniques if other_unique != unique\n",
    "            ):\n",
    "                return unique\n",
    "            else:\n",
    "                max_len = 0\n",
    "                for unique in uniques:\n",
    "                    new_len = sum(len(_) for _ in templates_uniques[unique])\n",
    "                    if new_len > max_len:\n",
    "                        max_len = new_len\n",
    "                        best = unique\n",
    "                return best\n",
    "    else:\n",
    "        pprint(templates_uniques)\n",
    "        raise RuntimeError(\n",
    "            \"Could not identify cluster of this line:\\n%s\\n%s\" %\n",
    "            (line, [templates_uniques[unique] for unique in uniques])\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def load_data(root_log_path, load_modified=False):\n",
    "    templates, logs = dict(), dict()\n",
    "    for template_filename in glob(\n",
    "        root_log_path + \"*/*log_templates%s.csv\" % (\n",
    "            \"_modified\" if load_modified else \"\"\n",
    "        )\n",
    "    ):\n",
    "        log_name = template_filename.split(\"/\")[-1].split(\"_2k\")[0]\n",
    "        print(\"Unpacking %s dataset ...\" % log_name, end='')\n",
    "        templates[log_name] = templates_from_csv(template_filename)\n",
    "        print(\"done\")\n",
    "        print(\"Checking %s integrity ....\" % log_name, end='')\n",
    "        with open(root_log_path + log_name + \"/\" + log_name + \"_2k.log\") as f:\n",
    "            logs[log_name] = f.read().splitlines()\n",
    "        for line in logs[log_name]:\n",
    "            get_cluster_id(line, templates[log_name])\n",
    "        print(\"OK\")\n",
    "        print(\"\")\n",
    "    return templates, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293e2f2-79e8-478f-ae29-57cb185c8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_LOG_PATH = \"./logs/\"\n",
    "\n",
    "# templates_dict, logs_dict = load_data(ROOT_LOG_PATH)\n",
    "templates_dict, logs_dict = load_data(ROOT_LOG_PATH, load_modified=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d4cec-a8a4-43d5-b60a-98abacc7faf1",
   "metadata": {},
   "source": [
    "# Loading the pattern collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512ce6a-3289-4e42-9ac5-e77094825855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_pattern_collection(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        pattern_collection = json.load(f)\n",
    "    return pattern_collection\n",
    "\n",
    "\n",
    "def to_logmine_params(pattern_collection: dict):\n",
    "    return [\n",
    "        '\"<%s>:/%s/\"' % (name, re.replace(\"[\", \"\\[\").replace(\"]\", \"\\]\")) \n",
    "        for name, re in pattern_collection.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "BASIC_COLLECTION = load_pattern_collection(\"./parameters/basic_collection.json\")\n",
    "SPECIFIC_COLLECTION = load_pattern_collection(\"./parameters/specific_collection.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d87f9e9-7d9d-4c1e-9f75-75598f8d421f",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e9403-1b1c-4800-83a1-0b4b745432ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "def get_ground_truth_as_list_of_sets(lines, templates):\n",
    "    map_cluster_index = defaultdict(set)\n",
    "    for i, line in enumerate(lines):\n",
    "        clust_id = get_cluster_id(line, templates)\n",
    "        map_cluster_index[clust_id].add(i)\n",
    "    return [v for v in map_cluster_index.values()]\n",
    "\n",
    "\n",
    "def get_ground_truth_as_list(lines, templates):\n",
    "    return [get_cluster_id(line, templates) for line in lines]\n",
    "\n",
    "\n",
    "def get_parsing_accuracy(obtained_clusters, lines, templates):\n",
    "    ground_truth_clusters = get_ground_truth_as_list_of_sets(lines, templates)\n",
    "    obtained_clusters = [set(cluster) for cluster in obtained_clusters]\n",
    "    clusters_in_common = [\n",
    "        c for c in obtained_clusters if c in ground_truth_clusters\n",
    "    ]\n",
    "    return sum(len(cluster) for cluster in clusters_in_common) / len(lines)\n",
    "\n",
    "\n",
    "def get_rand_index(obtained_clusters, lines, templates):\n",
    "    ground_truth_clusters = get_ground_truth_as_list(lines, templates)\n",
    "    obt_clust_as_list = [None for _ in range(len(lines))]\n",
    "    for i, cluster in enumerate(obtained_clusters):\n",
    "        for j in cluster:\n",
    "            obt_clust_as_list[j] = i\n",
    "    if not all(_ is not None for _ in obt_clust_as_list):\n",
    "        new_clust_id = max(\n",
    "            set([_ for _ in obt_clust_as_list if _ is not None])\n",
    "        ) + 1\n",
    "        obt_clust_as_list = [\n",
    "            _ if _ is not None else new_clust_id\n",
    "            for _ in obt_clust_as_list\n",
    "        ]\n",
    "    return adjusted_rand_score(ground_truth_clusters, obt_clust_as_list)\n",
    "\n",
    "\n",
    "TIME = \"time\"\n",
    "PA = \"parsing accuracy\"\n",
    "ARI = \"adjusted rand index\"\n",
    "NUM_CLUSTERS = \"number of clusters\"\n",
    "\n",
    "\n",
    "METRICS = {\n",
    "    \"parsing accuracy\": get_parsing_accuracy,\n",
    "    \"adjusted rand index\": get_rand_index\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d14e88-b15e-41cc-a13d-ad18f89340f3",
   "metadata": {},
   "source": [
    "# Logmine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c178a34-1576-4a73-a496-89f39b94cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import subprocess\n",
    "\n",
    "LOGMINE_REPO_PATH = \"../logmine\"\n",
    "\n",
    "\n",
    "def logmine_clustering(\n",
    "        logmine_repo_path: str,\n",
    "        file_path: str,\n",
    "        k1: float = None,\n",
    "        k2: float = None,\n",
    "        max_dist: float = None,\n",
    "        logmine_regexps: list = None,\n",
    "        verbose=False\n",
    "):\n",
    "    call_args = [logmine_repo_path + \"/logmine\"]\n",
    "    if k1 is not None:\n",
    "        call_args += [\"-k1\", str(k1)]\n",
    "    if k2 is not None:\n",
    "        call_args += [\"-k2\", str(k2)]\n",
    "    if max_dist is not None:\n",
    "        call_args += [\"-m\", str(max_dist)]\n",
    "    call_args += [\"-i\", \"1\"]\n",
    "    call_args += [file_path]\n",
    "\n",
    "    if len(logmine_regexps) > 0:\n",
    "        call_args += [\"-v\"] + logmine_regexps\n",
    "    logmine_output = subprocess.check_output(call_args).decode('utf-8')\n",
    "    if verbose:\n",
    "        print(logmine_output)\n",
    "    clusters = [\n",
    "        [int(x) for x in line.split(' ')]\n",
    "        for line in logmine_output.splitlines()\n",
    "    ]\n",
    "    return clusters\n",
    "\n",
    "def evaluate_logmine_clustering(\n",
    "        logmine_repo_path,\n",
    "        file_path,\n",
    "        ground_truth_templates,\n",
    "        metrics,\n",
    "        k1=None,\n",
    "        k2=None,\n",
    "        max_dist=None,\n",
    "        logmine_regexps=None,\n",
    "):\n",
    "    if logmine_regexps is None:\n",
    "        logmine_regexps = []\n",
    "    start = time()\n",
    "    clusters_as_list = logmine_clustering(\n",
    "        logmine_repo_path,\n",
    "        file_path,\n",
    "        k1,\n",
    "        k2,\n",
    "        max_dist,\n",
    "        logmine_regexps\n",
    "    )\n",
    "    computation_time = time() - start\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    results = {\n",
    "        name: metric(clusters_as_list, lines, ground_truth_templates)\n",
    "        for name, metric in metrics.items()\n",
    "    }\n",
    "    results[TIME] = computation_time\n",
    "    results[NUM_CLUSTERS] = len(clusters_as_list)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9a5f6b-7560-43f2-bbd5-fae9cc3ff9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_NAME = \"Proxifier\"\n",
    "MAX_DIST = 0.025\n",
    "\n",
    "file_path = ROOT_LOG_PATH + LOG_NAME + \"/\" + LOG_NAME + \"_2k.log\"\n",
    "\n",
    "for MAX_DIST in [0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.22, 0.24, 0.26, 0.28, 0.3, 0.32, 0.36, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]:\n",
    "    print(MAX_DIST, evaluate_logmine_clustering(\n",
    "        LOGMINE_REPO_PATH,\n",
    "        file_path,\n",
    "        templates_dict[LOG_NAME],\n",
    "        METRICS,\n",
    "        max_dist=MAX_DIST,\n",
    "        logmine_regexps=to_logmine_params(BASIC_COLLECTION)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82b8a2-6a34-4eff-b089-7d16f4d220a1",
   "metadata": {},
   "source": [
    "# Drain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa1fc7d-c632-4ef2-9b1b-99f1eadd4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "PATH_TO_DRAIN_CONFIG = \"./drain3config.ini\"\n",
    "\n",
    "\n",
    "def write_drain_config_file(\n",
    "    map_name_re,\n",
    "    extra_delimiters: list = [\"_\"],\n",
    "    sim_th: float = 0.4,\n",
    "    depth: int = 4,\n",
    "    max_children: int = 100,\n",
    "    max_clusters: int = 1024,\n",
    "    path_to_config: str = PATH_TO_DRAIN_CONFIG,\n",
    "):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(path_to_config)\n",
    "\n",
    "    config[\"MASKING\"][\"masking\"] = json.dumps([\n",
    "        {\"regex_pattern\": re, \"mask_with\": name}\n",
    "        for name, re in map_name_re.items()\n",
    "        if name != \"any\"\n",
    "    ])\n",
    "    config[\"DRAIN\"][\"sim_th\"] = str(sim_th)\n",
    "    config[\"DRAIN\"][\"depth\"] = str(depth)\n",
    "    config[\"DRAIN\"][\"max_children\"] = str(max_children)\n",
    "    config[\"DRAIN\"][\"max_clusters\"] = str(max_clusters)\n",
    "\n",
    "    with open(path_to_config, \"w\") as configfile:\n",
    "        config.write(configfile)\n",
    "try:\n",
    "    from drain3 import TemplateMiner\n",
    "    from drain3.template_miner_config import TemplateMinerConfig\n",
    "\n",
    "    def drain_clustering(\n",
    "            log_file_path: str,\n",
    "            map_name_re,\n",
    "            extra_delimiters: list = [\"_\"],\n",
    "            sim_th: float = 0.4,\n",
    "            depth: int = 4,\n",
    "            max_children: int = 100,\n",
    "            max_clusters: int = 1024,\n",
    "            path_to_config: str = PATH_TO_DRAIN_CONFIG,\n",
    "    ):\n",
    "        write_drain_config_file(\n",
    "            map_name_re,\n",
    "            extra_delimiters,\n",
    "            sim_th,\n",
    "            depth,\n",
    "            max_children,\n",
    "            max_clusters,\n",
    "            path_to_config\n",
    "        )\n",
    "\n",
    "        config = TemplateMinerConfig()\n",
    "        config.load(path_to_config)\n",
    "        config.profiling_enabled = False\n",
    "        template_miner = TemplateMiner(config=config)\n",
    "\n",
    "        with open(log_file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        clusters_as_dict = defaultdict(list)\n",
    "        for i, line in enumerate(lines):\n",
    "            cluster_id = template_miner.add_log_message(line)[\"cluster_id\"]\n",
    "            clusters_as_dict[cluster_id].append(i)\n",
    "        return clusters_as_dict\n",
    "except:\n",
    "    print(\"Warning: could not load Drain\")\n",
    "\n",
    "    def drain_clustering(\n",
    "            log_file_path: str,\n",
    "            map_name_re,\n",
    "            extra_delimiters: list = [\"_\"],\n",
    "            sim_th: float = 0.4,\n",
    "            depth: int = 4,\n",
    "            max_children: int = 100,\n",
    "            max_clusters: int = 1024,\n",
    "            path_to_config: str = PATH_TO_DRAIN_CONFIG,\n",
    "            show_clusters=False\n",
    "    ):\n",
    "        print(\"Error! Drain is not installed !!(https://github.com/IBM/Drain3)\")\n",
    "        \n",
    "\n",
    "def evaluate_drain_clustering(\n",
    "    log_file_path: str,\n",
    "    ground_truth_templates,\n",
    "    metrics,\n",
    "    map_name_re,\n",
    "    extra_delimiters: list = [\"_\"],\n",
    "    sim_th: float = 0.4,\n",
    "    depth: int = 4,\n",
    "    max_children: int = 100,\n",
    "    max_clusters: int = 1024,\n",
    "    path_to_config: str = PATH_TO_DRAIN_CONFIG,\n",
    "    show_clusters=False\n",
    "):\n",
    "    start = time()\n",
    "    clusters_as_dict = drain_clustering(\n",
    "        log_file_path,\n",
    "        map_name_re,\n",
    "        extra_delimiters,\n",
    "        sim_th,\n",
    "        depth,\n",
    "        max_children,\n",
    "        max_clusters,\n",
    "        path_to_config,\n",
    "        show_clusters=show_clusters,\n",
    "    )\n",
    "\n",
    "    clusters_as_list = [v for v in clusters_as_dict.values()]\n",
    "    computation_time = time() - start\n",
    "    with open(log_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    results = {\n",
    "        name: metric(clusters_as_list, lines, ground_truth_templates)\n",
    "        for name, metric in metrics.items()\n",
    "    }\n",
    "    results[TIME] = computation_time\n",
    "    results[NUM_CLUSTERS] = len(clusters_as_list)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bbbee9-6b65-409c-8848-7566e74b171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for DRAIN_SIM_TH in [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]:\n",
    "    for DRAIN_DEPTH in [3, 4, 5, 6]:\n",
    "        print(DRAIN_SIM_TH, DRAIN_DEPTH, evaluate_drain_clustering(\n",
    "            file_path,\n",
    "            templates_dict[LOG_NAME],\n",
    "            METRICS,\n",
    "            BASIC_COLLECTION,\n",
    "            sim_th=DRAIN_SIM_TH,\n",
    "            depth=DRAIN_DEPTH\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b10103-c5b8-4774-b9b1-c74b565eb20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
